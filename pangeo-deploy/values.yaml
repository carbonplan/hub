daskhub:
  rbac:
    enabled: true
  jupyterhub:
    hub:
      allowNamedServers: true
    singleuser:
      extraEnv:
        # The default worker image matches the singleuser image.
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'
        DASK_DISTRIBUTED__DASHBOARD_LINK: '/user/{JUPYTERHUB_USER}/proxy/{port}/status'
        DASK_LABEXTENSION__FACTORY__MODULE: 'dask_gateway'
        DASK_LABEXTENSION__FACTORY__CLASS: 'GatewayCluster'
      serviceAccountName: pangeo

    prePuller:
      hook:
        enabled: false

    scheduling:
      userScheduler:
        enabled: true
        replicas: 1
        resources:
          requests:
            memory: "32Mi"
            cpu: "10m"
          limits:
            memory: "128Mi"
            cpu: "500m"
      podPriority:
        enabled: true
      userPlaceholder:
        enabled: false
      extraConfig:
        optionHandler: |
          from dask_gateway_server.options import Options, Float, String, Mapping
          def cluster_options(user):
              def option_handler(options):
                  if ":" not in options.image:
                      raise ValueError("When specifying an image you must also provide a tag")
                  extra_annotations = {
                      "hub.jupyter.org/username": user.name,
                      "prometheus.io/scrape": "true",
                      "prometheus.io/port": "8787",
                  }
                  extra_labels = {
                      "hub.jupyter.org/username": user.name,
                  }
                  # We multiply the requests by 0.95 to ensure that that they
                  # pack well onto nodes. Kubernetes reserves a small fraction
                  # of the memory / CPU for itself, so the common situation of
                  # a node with 4 cores and a user requesting 4 cores means
                  # we request just over half of the *allocatable* CPU, and so
                  # we can't pack more than 1 worker on that node.
                  # On GCP, the kubernetes requests are ~12% of the CPU.
                  return {
                      "worker_cores": 0.88 * min(options.worker_cores / 2, 1),
                      "worker_cores_limit": options.worker_cores,
                      "worker_memory": "%fG" % (0.95 * options.worker_memory),
                      "worker_memory_limit": "%fG" % options.worker_memory,
                      "image": options.image,
                      "scheduler_extra_pod_annotations": extra_annotations,
                      "worker_extra_pod_annotations": extra_annotations,
                      "scheduler_extra_pod_labels": extra_labels,
                      "worker_extra_pod_labels": extra_labels,
                      "environment": options.environment,
                  }
              return Options(
                  Float("worker_cores", 2, min=1, max=16, label="Worker Cores"),
                  Float("worker_memory", 8, min=1, max=32, label="Worker Memory (GiB)"),
                  String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
                  Mapping("environment", {}, label="Environment Variables"),
                  handler=option_handler,
              )
          c.Backend.cluster_options = cluster_options
        idle: |
          # timeout after 30 minutes of inactivity
          c.KubeClusterConfig.idle_timeout = 1800
        limits: |
          # per Dask cluster limits.
          c.ClusterConfig.cluster_max_cores = 100
          c.ClusterConfig.cluster_max_memory = "600G"

homeDirectories:
  nfs:
    enabled: false
